<!-- Copyright (C) The IETF Trust (2014) -->
<!-- Copyright (C) The Internet Society (2014) -->

<section anchor="sec:fencing" title="Implementations in Existing Layout Types">
  <section anchor="sec:fencing:files" title="File Layout Type">
    <t>
      Not surprisingly, the File Layout Type comes closest to the
      normal semantics of NFSv4.1. In particular, the stateid used
      for I/O MUST have the same effect and be subject to
      the same validation on a data server as it would if the I/O was being
      performed on the metadata server itself in the absence of pNFS.
    </t>

    <t>
      And while for most implementations the storage devices
      can do the following validations:

      <list style='symbols'>
        <t>
          client holds a valid layout,
        </t>
        <t>
          client I/O matches the layout iomode,
        </t>
        <t>
          client does not go out of the byte ranges,
        </t>
      </list>

      these are each presented as a SHOULD and not a MUST. However,
      it is just these layout specific checks that are optional, not
      the normal file access semantics. The storage devices MUST
      make all of the required access checks on each READ or WRITE I/O as
      determined by the NFSv4.1 protocol.  If the metadata server would
      deny a READ or WRITE operation on a file due to its ACL, mode
      attribute, open access mode, open deny mode, mandatory byte-range
      lock state, or any other attributes and state, the storage device MUST
      also deny the READ or WRITE operation. And note that while the
      NFSv4.1 protocol does not mandate export access checks based on
      the client's IP address, if the metadata server implements such a
      policy, then that counts as such state as outlined above.
    </t>

    <t>
      As the data filehandle provided
      by the PUTFH operation and the stateid in the READ or WRITE operation
      are used to ensure that the client has a valid layout for the I/O
      being performed, the client can be fenced off for access to a specific
      file via the invalidation of either key.
    </t>
  </section>

  <section anchor="sec:fencing:blocks" title="Block Layout Type">
    <t>
      With the Block Layout Type, the storage devices are not guaranteed
      to be able to enforce file-based security.
      Typically, storage area network (SAN) disk arrays and SAN
      protocols provide access control mechanisms (e.g., Logical Unit
      Number (LUN) mapping and/or masking), which operate at the
      granularity of individual hosts, not individual blocks.
      Access to block storage is logically at a lower layer of the
      I/O stack than NFSv4, and hence NFSv4 security is not directly
      applicable to protocols that access such storage directly.
      As such, <xref target='RFC5663' />
      is very careful to define that in environments where pNFS
      clients cannot be trusted to enforce such policies, pNFS Block
      Layout Types SHOULD NOT be used.
    </t>

    <t>
      The implication here is that the security burden has shifted
      from the storage devices to the client.  It is the responsibility
      of the administrator doing the deployment to trust the client
      implementation. However, this is not a new requirement when it
      comes to SAN protocols, the client is expected to provide
      block-based protection.
    </t>

    <t>
      This implication also extends to ACLs, locks, and layouts. The
      storage devices might not be able to enforce any of these and
      the burden is pushed to the client to make the appropriate
      checks before sending I/O to the storage devices. As an example,
      if the metadata server uses a layout iomode for reading to
      enforce a mandatory read-only lock, then the client has to honor
      that intent by not sending WRITEs to the storage devices. The basic
      issue here is that once the client has access to the storage device
      it is able to write willy-nilly, even outside of the range of the
      layout.

      <cref anchor='AI1' source='TH'>  Willy-nilly means I want to craft this better!</cref>
    </t>

    <t>
      While the Block Layout Type does support client fencing upon
      revoking a layout, the above restrictions come into play again:
      the granularity of the fencing can only be at the host/logical-unit
      level. Thus, if one of a client's layouts is unilaterally revoked
      by the server, it will effectively render useless *all* of the
      client's layouts for files located on the storage units comprising
      the logical volume.  This may render useless the client's layouts
      for files in other file systems.
    </t>
  </section>

  <section anchor="sec:fencing:objects" title="Object Layout Type">
    <t>
      <list style='format (%d)'>
        <t>
          The object-based metadata server should recall outstanding layouts in
   the following cases: When the file's security policy changes, i.e., Access Control
      Lists (ACLs) or permission mode bits are set.
        </t>
        <t>
   The metadata server enforces the file access-control policy at
   LAYOUTGET time.  The client should use suitable authorization
   credentials for getting the layout for the requested iomode (READ or
   RW) and the server verifies the permissions and ACL for these
   credentials, possibly returning NFS4ERR_ACCESS if the client is not
   allowed the requested iomode.  If the LAYOUTGET operation succeeds
   the client receives, as part of the layout, a set of object
   capabilities allowing it I/O access to the specified objects
   corresponding to the requested iomode.  When the client acts on I/O
   operations on behalf of its local users, it MUST authenticate and
   authorize the user by issuing respective OPEN and ACCESS calls to the
   metadata server, similar to having NFSv4 data delegations.  If access
   is allowed, the client uses the corresponding (READ or RW)
   capabilities to perform the I/O operations at the object storage
   devices.  When the metadata server receives a request to change a
   file's permissions or ACL, it SHOULD recall all layouts for that file
   and it MUST change the capability version attribute on all objects
   comprising the file to implicitly invalidate any outstanding
   capabilities before committing to the new permissions and ACL.  Doing
   this will ensure that clients re-authorize their layouts according to
   the modified permissions and ACL by requesting new layouts.
   Recalling the layouts in this case is courtesy of the server intended
   to prevent clients from getting an error on I/Os done after the
   capability version changed.
        </t>
        <t>
   Since capabilities are tied to layouts, and since they are used to
   enforce access control, when the file ACL or mode changes the
   outstanding capabilities MUST be revoked to enforce the new access
   permissions.  The server SHOULD recall layouts to allow clients to
   gracefully return their capabilities before the access permissions
   change.
        </t>
        <t>
   At any time, the metadata server may invalidate all outstanding
   capabilities on an object by changing its POLICY ACCESS TAG
   attribute.  The value of the POLICY ACCESS TAG is part of a
   capability, and it must match the state of the object attribute.  If
   they do not match, the OSD rejects accesses to the object with the
   sense key set to ILLEGAL REQUEST and an additional sense code set to
   INVALID FIELD IN CDB.  When a client attempts to use a capability and
   is rejected this way, it should issue a LAYOUTCOMMIT for the object
   and specify PNFS_OSD_BAD_CRED in the olr_ioerr_report parameter.  The
   client may elect to issue a compound LAYOUTRETURN/LAYOUTGET (or
   LAYOUTCOMMIT/LAYOUTRETURN/LAYOUTGET) to attempt to fetch a refreshed
   set of capabilities.
   <vspace blankLines="1"/>
   The metadata server may elect to change the access policy tag on an
   object at any time, for any reason (with the understanding that there
   is likely an associated performance penalty, especially if there are
   outstanding layouts for this object).  The metadata server MUST
   revoke outstanding capabilities when any one of the following occurs:
   <vspace blankLines="1"/>
   o  the permissions on the object change,
   <vspace blankLines="1"/>
   o  a conflicting mandatory byte-range lock is granted, or
   <vspace blankLines="1"/>
   o  a layout is revoked and reassigned to another client.
   <vspace blankLines="1"/>
   A pNFS client will typically hold one layout for each byte range for
   either READ or READ/WRITE.  The client's credentials are checked by
   the metadata server at LAYOUTGET time and it is the client's
   responsibility to enforce access control among multiple users
   accessing the same file.  It is neither required nor expected that
   the pNFS client will obtain a separate layout for each user accessing
     shared object.  The client SHOULD use OPEN and ACCESS calls to
   check user permissions when performing I/O so that the server's
   access control policies are correctly enforced.  The result of the
   ACCESS operation may be cached while the client holds a valid layout
   as the server is expected to recall layouts when the file's access
   permissions or ACL change.
        </t>
      </list>
    </t>
  </section>
</section>

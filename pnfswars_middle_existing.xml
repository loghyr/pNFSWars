<!-- Copyright (C) The IETF Trust (2014) -->
<!-- Copyright (C) The Internet Society (2014) -->

<section anchor="sec:fencing" title="Implementations in Existing Layout Types">
  <section anchor="sec:fencing:files" title="File Layout Type">
    <t>
      Not surprisingly, the file layout type comes closest to the
      normal semantics of NFSv4.1. In particular, the stateid used
      for I/O MUST have the same effect and be subject to
      the same validation on a data server as it would if the I/O was being
      performed on the metadata server itself in the absence of pNFS.
    </t>

    <t>
      And while for most implementations the storage devices
      can do the following validations:

      <list style='symbols'>
        <t>
          client holds a valid layout,
        </t>
        <t>
          client I/O matches the layout iomode, and,
        </t>
        <t>
          client does not go out of the byte ranges,
        </t>
      </list>

      these are each presented as a "SHOULD" and not a "MUST". However,
      it is just these layout specific checks that are optional, not
      the normal file access semantics. The storage devices MUST
      make all of the required access checks on each READ or WRITE I/O as
      determined by the NFSv4.1 protocol.  If the metadata server would
      deny a READ or WRITE operation on a file due to its ACL, mode
      attribute, open access mode, open deny mode, mandatory byte-range
      lock state, or any other attributes and state, the storage device MUST
      also deny the READ or WRITE operation. And note that while the
      NFSv4.1 protocol does not mandate export access checks based on
      the client's IP address, if the metadata server implements such a
      policy, then that counts as such state as outlined above.
    </t>

    <t>
      As the data filehandle provided
      by the PUTFH operation and the stateid in the READ or WRITE operation
      are used to ensure that the client has a valid layout for the I/O
      being performed, the client can be fenced off for access to a specific
      file via the invalidation of either key.
    </t>
  </section>

  <section anchor="sec:fencing:blocks" title="Block Layout Type">
    <t>
      With the block layout type, the storage devices are not guaranteed
      to be able to enforce file-based security.
      Typically, storage area network (SAN) disk arrays and SAN
      protocols provide access control mechanisms (e.g., Logical Unit
      Number (LUN) mapping and/or masking), which operate at the
      granularity of individual hosts, not individual blocks.
      Access to block storage is logically at a lower layer of the
      I/O stack than NFSv4, and hence NFSv4 security is not directly
      applicable to protocols that access such storage directly.
      As such, <xref target='RFC5663' />
      is very careful to define that in environments where pNFS
      clients cannot be trusted to enforce such policies, pNFS block
      layout types SHOULD NOT be used.
    </t>

    <t>
      The implication here is that the security burden has shifted
      from the storage devices to the client.  It is the responsibility
      of the administrator doing the deployment to trust the client
      implementation. However, this is not a new requirement when it
      comes to SAN protocols, the client is expected to provide
      block-based protection.
    </t>

    <t>
      This implication also extends to ACLs, locks, and layouts. The
      storage devices might not be able to enforce any of these and
      the burden is pushed to the client to make the appropriate
      checks before sending I/O to the storage devices. As an example,
      if the metadata server uses a layout iomode for reading to
      enforce a mandatory read-only lock, then the client has to honor
      that intent by not sending WRITEs to the storage devices. The basic
      issue here is that the storage device can be treated as a local dumb disk
      such that once the client has access to the storage device,
      it is able to perform either READ or WRITE I/O to the entire
      storage device. The byte ranges in the layout, any locks, the
      layout iomode, etc, can only be enforced by the client.
    </t>

    <t>
      While the block layout type does support client fencing upon
      revoking a layout, the above restrictions come into play again:
      the granularity of the fencing can only be at the host/logical-unit
      level. Thus, if one of a client's layouts is unilaterally revoked
      by the server, it will effectively render useless *all* of the
      client's layouts for files located on the storage units comprising
      the logical volume.  This may render useless the client's layouts
      for files in other file systems.
    </t>
  </section>

  <section anchor="sec:fencing:objects" title="Object Layout Type">
    <t>
      The object layout type focuses security checks to occur during
      the allocation of the layout. The client will typically ask
      for a layout for each byte-range of either READ or READ/WRITE.
      At that time, the metadata server should verify permissions
      against the layout iomode, the outstanding locks, the file
      mode bits or ACLs, etc. As the client may be acting for
      multiple local users, it MUST authenticate and authorize the
      user by issuing respective OPEN and ACCESS calls to the
      metadata server, similar to having NFSv4 data delegations.
    </t>

    <t>
      Upon successful authorization, inside the layout, the client
      receives a set of object capabilities allowing it I/O
      access to the specified objects corresponding to the requested
      iomode. These capabilities are used to enforce access control
      at the storage devices.  Whenever the metadata server detects one of:

      <list style='symbols'>
        <t>
          the permissions on the object change,
        </t>
        <t>
          a conflicting mandatory byte-range lock is granted, or
        </t>
        <t>
          a layout is revoked and reassigned to another client,
        </t>
      </list>

      then it MUST change the capability version attribute on all objects
      comprising the file to implicitly invalidate any outstanding
      capabilities before committing to one of these changes.
    </t>

    <t>
      When the metadata server wishes to fence off a client to a particular
      object, then it can use the above approach to invalidate
      the capability attribute on the given object. The client can
      be informed via the storage device that the capability has been
      rejected and is allowed to fetch a refreshed set of capabilities,
      i.e., re-acquire the layout.
    </t>
  </section>
</section>

<!-- Copyright (C) The IETF Trust (2014) -->
<!-- Copyright (C) The Internet Society (2014) -->

<section anchor="sec:fencing" title="Specifications of Existing Layout Types">
  <t>
    This section is not normative with regards to each of the presented
    types. This document does not update the specification of either
    the block layout type (see <xref target='RFC5663' />) or the object
    layout type (see <xref target='RFC5664' />).  Nor does it update
    Section 13 of <xref target='RFC5661' />, but rather Section 12 of
    that document. In other words, it is the pNFS requirements being
    updated, not the specification of the file layout type.
  </t>

  <section anchor="sec:fencing:files" title="File Layout Type">
    <t>
      Because the storage protocol is a subset of NFSv4.1, the semantics
      of the file layout type comes closest to the semantics of NFSv4.1
      in the absence of pNFS.  In particular, the stateid and principal
      used for I/O MUST have the same effect and be subject to the
      same validation on a data server as it would if the I/O were
      being performed on the metadata server itself.  The same set of
      validations apply whether pNFS is in effect or not.
    </t>

    <t>
      And while for most implementations the storage devices
      can do the following validations:

      <list style='format (%d)'>
        <t>
          client holds a valid layout,
        </t>
        <t>
          client I/O matches the layout iomode, and,
        </t>
        <t>
          client does not go out of the byte ranges,
        </t>
      </list>

      these are each presented as a "SHOULD" and not a "MUST". Actually,
      the first point is presented as both:

      <list style='hanging'>
        <t hangText='"MUST":'>
          in Section 13.6 of <xref target='RFC5661' />
          <vspace blankLines='1' />
          "As described in Section 12.5.1, a client MUST NOT send an I/O
          to a data server for which it does not hold a valid layout;
          the data server MUST reject such an I/O."
        </t>

        <t hangText='"SHOULD":'>
          in Section 13.8 of <xref target='RFC5661' />
          <vspace blankLines='1' />

          "The iomode need not be checked by the data servers when clients
          perform I/O.  However, the data servers SHOULD still validate
          that the client holds a valid layout and return an error if
          the client does not."
        </t>
      </list>

      However, it is just these layout specific checks that are optional,
      not the normal file access semantics. The storage devices MUST
      make all of the required access checks on each READ or WRITE I/O
      as determined by the NFSv4.1 protocol.  If the metadata server
      would deny a READ or WRITE operation on a file due to its ACL, mode
      attribute, open access mode, open deny mode, mandatory byte-range
      lock state, or any other attributes and state, the storage device
      MUST also deny the READ or WRITE operation. And note that while
      the NFSv4.1 protocol does not mandate export access checks based
      on the client's IP address, if the metadata server implements such
      a policy, then that counts as such state as outlined above.
    </t>

    <t>
      The data filehandle provided by the PUTFH operation to the
      data server is sufficient to ensure that for the subsequent
      READ or WRITE operation in the compound, that the client has a
      valid layout for the I/O being performed.
    </t>

    <t>
      Finally, the data server can check the stateid presented in the READ
      or WRITE operation to see if that stateid has been rejected by the
      metadata server such to cause the I/O to be fenced. Whilst it might
      just be the open owner or lock owner on that client being fenced,
      the client should take the NFS4ERR_BAD_STATEID error code to mean
      it has been fenced from the file and contact the metadata server.
    </t>
  </section>

  <section anchor="sec:fencing:blocks" title="Block Layout Type">
    <t>
      With the block layout type, the storage devices are not guaranteed
      to be able to enforce file-based security.
      Typically, storage area network (SAN) disk arrays and SAN
      protocols provide access control mechanisms (e.g., Logical Unit
      Number (LUN) mapping and/or masking), which operate at the
      granularity of individual hosts, not individual blocks.
      Access to block storage is logically at a lower layer of the
      I/O stack than NFSv4, and hence NFSv4 security is not directly
      applicable to protocols that access such storage directly.
      As such, Section 2.1 <xref target='RFC5663' />
      specifies that:

      <list style='hanging'>
        <t hangText=''>
          "in environments where pNFS clients cannot be trusted to enforce
          such policies, pNFS block layout types SHOULD NOT be used."
        </t>
      </list>
    </t>

    <t>
      As a result of these granularity issues, the security burden
      has been shifted from the storage devices to the client.  Those
      deploying implementations of this layout type need to be sure that
      the client implementation can be trusted  This is not a new sort of
      requirement in the context of SAN protocols.  In such environments,
      the client is expected to provide block-based protection.
    </t>

    <t>
      This shift of the burden also extends to ACLs, locks, and
      layouts. The storage devices might not be able to enforce any of
      these and the burden is pushed to the client to make the appropriate
      checks before sending I/O to the storage devices. As an example,
      if the metadata server uses a layout iomode for reading to enforce
      a mandatory read-only lock, then the client has to honor that
      intent by not sending WRITEs to the storage devices. The basic
      issue here is that the storage device can be treated as a local
      dumb disk such that once the client has access to the storage
      device, it is able to perform either READ or WRITE I/O to the
      entire storage device. The byte ranges in the layout, any locks,
      the layout iomode, etc, can only be enforced by the client.
    </t>

    <t>
      While the block layout type does support client fencing upon
      revoking a layout, the above restrictions come into play again:
      the granularity of the fencing can only be at the host/logical-unit
      level. Thus, if one of a client's layouts is unilaterally revoked
      by the server, it will effectively render useless *all* of the
      client's layouts for files located on the storage units comprising
      the logical volume.  This may render useless the client's layouts
      for files in other file systems.
    </t>
  </section>

  <section anchor="sec:fencing:objects" title="Object Layout Type">
    <t>
      The object layout type focuses security checks to occur during
      the allocation of the layout. The client will typically ask
      for a layout for each byte-range of either READ or READ/WRITE.
      At that time, the metadata server should verify permissions
      against the layout iomode, the outstanding locks, the file
      mode bits or ACLs, etc. As the client may be acting for
      multiple local users, it MUST authenticate and authorize the
      user by issuing respective OPEN and ACCESS calls to the
      metadata server, similar to having NFSv4 data delegations.
    </t>

    <t>
      Upon successful authorization, inside the layout, the client
      receives a set of object capabilities allowing it I/O
      access to the specified objects corresponding to the requested
      iomode. These capabilities are used to enforce access control
      at the storage devices.  Whenever the metadata server detects one of:

      <list style='symbols'>
        <t>
          the permissions on the object change,
        </t>
        <t>
          a conflicting mandatory byte-range lock is granted, or
        </t>
        <t>
          a layout is revoked and reassigned to another client,
        </t>
      </list>

      then it MUST change the capability version attribute on all objects
      comprising the file to implicitly invalidate any outstanding
      capabilities before committing to one of these changes.
    </t>

    <t>
      When the metadata server wishes to fence off a client to a particular
      object, then it can use the above approach to invalidate
      the capability attribute on the given object. The client can
      be informed via the storage device that the capability has been
      rejected and is allowed to fetch a refreshed set of capabilities,
      i.e., re-acquire the layout.
    </t>
  </section>
</section>
